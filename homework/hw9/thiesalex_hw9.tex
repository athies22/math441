%!TEX output_directory = temp
\documentclass[letterpaper, 12pt]{amsart}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%% boilerplate packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\usepackage[margin=2in]{geometry}
		\usepackage{amsmath,amssymb,amsthm}
		\usepackage{marvosym}
		\usepackage[mathscr]{euscript}
		\usepackage{enumerate}
		\usepackage{graphicx}
		\usepackage{mathrsfs}
		\usepackage{color}
		\usepackage{hyperref}
		\usepackage{verbatim}
		\usepackage{stmaryrd}
		\usepackage{endnotes}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%% rename the abstract %%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\renewcommand{\abstractname}{Assignment}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% sets %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\DeclareMathOperator{\N}{\mathbb{N}}				% natural numbers
		\DeclareMathOperator{\Z}{\mathbb{Z}}				% integers
		\DeclareMathOperator{\Zp}{\mathbb{Z}^{+}}			% positive integers
		\DeclareMathOperator{\Q}{\mathbb{Q}}				% rationals
		\DeclareMathOperator{\Qc}{\mathbb{Q}^{c}}			% irrationals
		\DeclareMathOperator{\R}{\mathbb{R}}				% reals
		\DeclareMathOperator{\F}{\mathbb{F}}				% a field
		\DeclareMathOperator{\C}{\mathbb{C}}				% complex numbers
		\DeclareMathOperator{\Cnon}{\mathbb{C}^{\times}}	% nonzero complex numbers
		\DeclareMathOperator{\Pcal}{\mathcal{P}}			% powerset, or set of polynomials
		\DeclareMathOperator{\Ell}{\mathscr{L}}				% set of linear maps, or linear operator

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% use pretty letters %%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\DeclareMathOperator{\ep}{\varepsilon}				% epsilons
		\DeclareMathOperator{\ph}{\varphi}					% phis

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% algebra %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\renewcommand{\null}{\text{null }}					% null space
		\DeclareMathOperator{\range}{\text{range }}			% range
		\newcommand{\bmat}[1]{{\mathbf{#1}}}				% bold matrix
		\newcommand{\bvec}[1]{{\vec{\mathbf{#1}}}}			% bold vector
		\DeclareMathOperator{\ind}{\perp\!\!\!\perp}		% perpendicular, orthogonal
		\DeclareMathOperator{\ord}{\text{ord}}				% order of a structure
		\DeclareMathOperator{\Log}{Log}						% logarithm
		\DeclareMathOperator{\Span}{Span}					% span
		\newcommand{\pid}[1]{\langle #1 \rangle}			% bracket notation, used for 
															% ideals or inner products
		\newcommand{\norm}[1]{\mid \!\!#1 \!\!\mid}			%\norm{x} gives |x|

		% fatdot notation
		\makeatletter
			\newcommand*\fatdot{\mathpalette\fatdot@{.5}}
			\newcommand*\fatdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
		\makeatother

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%% probability & statistics %%%%%%%%%%%%%%%%%%%%%%%%%
		\renewcommand{\Pr}{\mathbb{P}}						% probability
		\DeclareMathOperator{\E}{\mathbb{E}}				% expectation
		\DeclareMathOperator{\var}{\rm Var}					% variance
		\DeclareMathOperator{\sd}{\rm SD}					% standard deviation
		\DeclareMathOperator{\cov}{\rm Cov}					% covariance
		\DeclareMathOperator{\SE}{\rm SE}					% standard error
		\DeclareMathOperator{\ssreg}{{\rm SS}_{{\rm Reg}}}	% sum of squared regression
		\DeclareMathOperator{\ssr}{{\rm SS}_{{\rm Res}}}	% sum of squared residuals
		\DeclareMathOperator{\sst}{{\rm SS}_{{\rm Tot}}}	% total sum of squares

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% number theory %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\renewcommand{\mod}[1]{\ (\mathrm{mod}\ #1)}		% congruences

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%% theorem environments %%%%%%%%%%%%%%%%%%%%%%%%%%%%
		% Some theorem-like environments, all numbered together starting at 1
		% in each section.

		\newtheorem{thm}{Theorem}[section]					% The default \theoremstyle is 
		\newtheorem{defn}[thm]{Definition}					% bold headings and italic body text.
		\newtheorem{prop}[thm]{Proposition}
		\newtheorem{claim}[thm]{Claim}
		\newtheorem{cor}[thm]{Corollary}
		\newtheorem{lemma}[thm]{Lemma}

		\theoremstyle{definition}  							% Bold headings and Roman body text.
		\newtheorem{example}[thm]{Example}
		\newtheorem{examples}[thm]{Examples}
		\newtheorem{exercise}[thm]{Exercise}
		\newtheorem{note}[thm]{Note}
		\newtheorem{remark}[thm]{Remark}
		\newtheorem{remarks}[thm]{Remarks}
		\newtheorem{discussion}[thm]{Discussion}

		\newcommand{\dfn}{\textbf} 							% Make defined words bold.
		\newcommand{\mdfn}[1]{\dfn{\mathversion{bold}#1}} 	% Even make math symbols bold

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% complex numbers %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\DeclareMathOperator{\Arg}{Arg}						% argument of z \in \C
		\DeclareMathOperator{\re}{Re}						% real component
		\DeclareMathOperator{\im}{Im}						% imaginary component

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% various symbols %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\newcommand{\iso}{\cong}						% isometric/congruent
		\newcommand{\ra}{\rightarrow}                   % right arrow
		\newcommand{\Ra}{\Rightarrow}                   % right implies
		\newcommand{\lra}{\longrightarrow}              % long right arrow
		\newcommand{\la}{\leftarrow}                    % left arrow
		\newcommand{\La}{\Leftarrow}                    % left implies
		\newcommand{\lla}{\longleftarrow}               % long left arrow
		\newcommand{\eqra}{\llra{\sim}}                 % equivalence/isomorphism
		\newcommand{\blank}{\underbar{\ \ }}          	% An underscore, as in (__)xV
		% \newcommand{\blank}{-}                          % A hyphen, as in (-)xV
		\newcommand{\Id}{Id}                            % The identity functor
		\newcommand{\und}{\underline}
		\newcommand{\del}{\nabla}						% gradient vector

		\raggedbottom		
\begin{document}
	\title{Homework 9  -- Math 441 \\ \today}
	\author{Alex Thies \\ \href{mailto:athies@uoregon.edu}{\lowercase{athies$@$uoregon.edu}}}

	\begin{abstract}
	The following exercises are assigned from \textit{Linear Algebra Done Right}, 3rd Edition, by Sheldon Axler. 
			\begin{tabular}{rl}
				& 5.B - 15, 20; \\
				& 5.C - 1, 3, 6, 8, 12, 14; \\
				& 8.A - 2, 6, 12, 17.
			\end{tabular}
	\end{abstract}

	\maketitle

	\section*{Section 5.B}
		\subsection*{Exercise 15}
		Give an example of an operator whose matrix with respect to some basis contains only nonzero numbers on the diagonal, but the operator is not invertible.
		\vspace*{3mm}

		\begin{proof}
		This is actually pretty simple if we restrict ourselves to two-dimensional vector spaces and think about silly looking matrices.
		We'll pick $T$ so that its diagonal is just a string of 1's, and then pick the other entries to interfere with either injectivity or surjectivity, since they are each equivalent to invertibility.
		Let $V = \R^{2}$, $e_{1},e_{2}$ be the standard basis of $\R^2$, and $T \in \Ell(V)$ such that $$\mathcal{M}(T; e_{1},e_{2}) = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$$
		Then we compute the following,
			\begin{align*}
				Te_{1} &= \mathcal{M}(T)\mathcal{M}(e_{1}), \\
				&= \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
			\end{align*}
			\begin{align*}
				Te_{2} &= \mathcal{M}(T)\mathcal{M}(e_{2}), \\
				&= \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
			\end{align*}
		It follows that $\range T \neq V$, hence $T$ is not surjective, implying that it is also not invertible, as desired.

		Notice that we could extend this example to all scalar multiples of $\mathcal{M}(T; e_{1},e_{2})$.
		Further, if we allow ourselves to think about row-reduction and determinants, then we know that we could extend this example to all square matrices (i.e., linear operators represented as square matrices) where each entry is equal to the same $\alpha \in \F/\{0\}$.
		\end{proof}
		% subsection exercise_15 (end)
		\vspace*{2mm}
		\hrule
		\vspace*{2mm}

		\subsection*{Exercise 20}
		Suppose $V$ is a finite-dimensional complex vector space and $T \in \Ell(V)$. 
		Prove that $T$ has an invariant subspace of dimension $k$ for each $k = 1, \dots, \dim{V}$.
		\vspace*{3mm}

		\begin{proof}
		Let $V$ and $T$ be as above, let $n = \dim V$.
		Since $V$ is finite-dimensional, it has a basis $v_{1}, \dots, v_{n}$.
		In fact it has many bases.
		Use Thereom 5.26(a) to pick a basis $v_{1}, \dots, v_{n}$ such that $\mathcal{M}(T; v_{1}, \dots, v_{n})$ is upper-triangular.
		Well, by Theorem 5.26(c), we also have that $\Span(v_{1}, \dots, v_{j})$ is invariant under $T$ for each $j = 1, \dots, n$.
		It follows by definition that each of the invariant subsapces $\Span(v_{1}, \dots, v_{j})$ has dimension $j$, which is what we set out to prove.
		If we switch our $j$'s for $k$'s, then we have shown $T$ has an invariant subspace of dimension $k$ for each $k = 1, \dots, \dim{V}$, as desired.
		\end{proof}
		% subsection exercise_20 (end)
		\vspace*{2mm}
		\hrule
		\vspace*{2mm}
	% section section_5_b (end)

	\section*{Section 5.C}
		\subsection*{Exercise 1}
		Suppose $T \in \Ell(V)$ is diagonalizable. 
		Prove that $V = \null T \oplus \range T$.
		\vspace*{3mm}

		\begin{proof}
		Let $T \in \Ell(V)$ such that $T$ is diagonalizable.
		By definition, we can say that $V$ is finite-dimensional.

		First, some boring cases.
		Suppose $\null T = \{ 0 \}$.
		Then by Theorem 3.16 we have that $T$ is injective, and since $T$ is an operator, by Theorem 3.69 it is also surjective.
		It follows that $\range T = V$.
		Moreover, we have $\null T \cap \range T = \{ 0 \}$, so we can invoke Theorem 1.45 to claim that $\null T + \range T$ is a direct sum.
		Further, Since $T$ is a bijection, it follows that $V = \null T \oplus \range T$, like we want.
		Next, suppose $\range T = \{ 0 \}$.
		This implies that $T$ is the zero map, in which case $\null T = V$.
		\textit{Mutatis mutandis} and we have $V = \null T \oplus \range T$, again.

		Now, with the trivial cases out of the way, assume $\range T$ and $\null T$ each have as elements some nonzero vectors.
		Hence, we have $\null T \neq 0$, implying that $T$ is not injective, and therefore not invertible.
		Then it follows that $T$ has $\lambda_{1} = 0$ for an eigenvalue.
		Let $\lambda_{2}, \dots, \lambda_{n}$ be the remaining distinct eigenvalues of $T$.
		Then we have the eigenspaces, $$E(\lambda_{1}, T), E(\lambda_{2},T), \dots, E(\lambda_{n}, T)$$
		Notice that $E(0,T) = \null T$, further by the definitions of eigen-
		spaces, we know $\bigcap_{i=1}^{n} E(\lambda_{i},T) = \{ 0 \}$ which implies that $$E(\lambda_{1}, T) + E(\lambda_{2},T) + \cdots + E(\lambda_{n}, T),$$ is a direct sum.
		So by Theorem 5.41, it makes sense to write,
			\begin{align*}
				V &= E(\lambda_{1}, T) \oplus E(\lambda_{2},T) \oplus \cdots \oplus E(\lambda_{n}, T), \\
				&= E(0, T) \oplus E(\lambda_{2},T) \oplus \cdots \oplus E(\lambda_{n}, T), \\
				&= \null T \oplus \left[ E(\lambda_{2},T) \oplus \cdots \oplus E(\lambda_{n}, T) \right].
			\end{align*}
		Thus, to prove $V = \null T \oplus \range T$, it will suffice to show, $$\range T = E(\lambda_{2},T) \oplus \cdots \oplus E(\lambda_{n}, T).$$
		We do this by double-inclusion.

		Let $u \in \range T$.
		Recall two things.
		First, since $E(0,T) = \null T$, and operators map $0 \mapsto 0$, we have $\range T|_{E(\lambda_{2},T) \oplus \cdots \oplus E(\lambda_{n}, T)} = \range T$.
		Second, the corresponding eigenvectors $v_{2}, \dots, v_{n}$ for the distinct eigenvalues $\lambda_{2}, \dots, \lambda_{n}$ are linearly independent and form a basis of $V$.
		Therefore, for each $u \in \range T$ we have $v_{2}, \dots, v_{n}$ such that $u = T(v_{2}, \dots, v_{n})$.
		Then, since each $v_{j} \in E(\lambda_{j},T)$ for $j = 2,3,\dots,n$, we have $u \in E(\lambda_{2},T) \oplus \cdots \oplus E(\lambda_{n}, T)$.
		It follows that $\range T \subset E(\lambda_{2},T) \oplus \cdots \oplus E(\lambda_{n}, T)$, as desired.

		To prove the reverse inclusion, let $w \in E(\lambda_{2},T) \oplus \cdots \oplus E(\lambda_{n}, T)$.
		Then $w = w_{2} + \cdots + w_{n}$ where each $w_{j} \in E(\lambda_{j},T)$ for $j = 2,3,\dots,n$.
		Thus, each $w_{j}$ is an eigenvector for $\lambda_{j}$, i.e., $$Tw_{j} = \lambda_{j}w_{j}.$$
		Eigenspaces are closed under scalar multiplication, so we can make $\tfrac{1}{\lambda_{j}}w_{j} \in E(\lambda_{j},T)$.
		We need these terms because they have the nice behavior exhibited below,
			\begin{align*}
				w_{j} &= \frac{\lambda_{j}}{\lambda_{j}} w_{j}, \\
				&= \lambda_{j}\left(\tfrac{1}{\lambda_{j}} w_{j} \right), \\
				&= T\left(\tfrac{1}{\lambda_{j}} w_{j} \right).
			\end{align*}
		This allows us to take $w = w_{2} + w_{3} + \cdots$, and write it as $$w = T\left(\tfrac{1}{\lambda_{2}} w_{2}\right) + T\left(\tfrac{1}{\lambda_{3}} w_{3}\right) + \cdots + T\left(\tfrac{1}{\lambda_{n}} w_{n}\right).$$
		This implies $w \in \range T$, hence $E(\lambda_{2},T) \oplus \cdots \oplus E(\lambda_{n}, T) \subset \range T$.
		Thus, we have shown $\range T = E(\lambda_{2},T) \oplus \cdots \oplus E(\lambda_{n}, T)$, and more importantly, $V = \null T \oplus \range T$ as we wanted to prove.
		\end{proof}
		% subsection exercise_1 (end)
		\vspace*{2mm}
		\hrule
		\vspace*{2mm}

		\subsection*{Exercise 3}
		Suppose $V$ is finite-dimensional and $T \in \Ell(V)$. 
		Prove that the following are equivalent:
			\begin{enumerate}[\hspace*{5mm}(a)]
				\item $V = \null T \oplus \range T$.
				\item $V = \null T + \range T$.
				\item $\null T \cap \range T = \{0\}$.
			\end{enumerate}
		\vspace*{3mm}

		\begin{proof}
		Let $V,T$ be as above.
		Since $V$ is finite-dimensional, we have several useful theorems.
		To prove the above are equivalent conditions we will show $(a) \Ra (b)$, $(b) \Ra (c)$, and $(c) \Ra (a)$.
		By transitivity, this implies they are each equivalent to one another.

		\textbf{(a)$\Ra$(b)}.
		Assume $V = \null T \oplus \range T$; then by definition $V = \null T + \range T$.
		One down, two to go.

		\textbf{(b)$\Ra$(c)}.
		Assume $V = \null T + \range T$.
		Since $V$ is finite-dimensional, it follows by the Fundamental Theorem of Linear Maps that $\dim V \geq \dim \null T + \dim \range T$.
		Moreover, by Theorem 2.34 we have $\dim(\null T + \range T) = \dim{\null T} + \dim{\range T} - \dim{(\null{T}\cap\range{T})}$.
		By our assumption $$\dim(\null T + \range T) = \dim{V},$$ allowing us to compute,
			\begin{align*}
				\dim V &\geq \dim \null T + \dim \range T, \\
				\dim(\null T + \range T) &\geq \dim \null T + \dim \range T, \\
				\dim{\null T} + \dim{\range T} &\geq \dim \null T + \dim \range T, \\
				- \dim{(\null{T}\cap\range{T})} & \\
				0 &\geq \dim{(\null{T}\cap\range{T})}.
			\end{align*}
		It follows that $\dim{(\null{T}\cap\range{T})} = 0$, hence we have the desired result that $\null T \cap \range T = \{0\}$.
		One piece to go.

		\textbf{(c)$\Ra$(a)}.
		Assume $\null T \cap \range T = \{0\}$.
		Our first conclusion is that $\null T + \range T$ is a direct sum, by Theorem 1.45.
		Since $\dim{(\null T \cap \range T)} = 0$, and by our previously used theorems we have,
			\begin{align*}
			\dim{V} &= \dim(\null T + \range T), \\
			&= \dim{\null T} + \dim{\range T} - \dim{(\null{T}\cap\range{T})}, \\
			&= \dim{\null T} + \dim{\range T} - 0, \\
			&= \dim{\null T} + \dim{\range T}.
			\end{align*}
		It follows that $\null T \oplus \range T$ is of the correct dimension for us to claim our final result, that $V = \null T \oplus \range T$.
		
		Thus, by the transitivity of implications, we have shown that (a), (b), and (c) are equivalent.
		\end{proof}
		% subsection exercise_3 (end)
		\vspace*{2mm}
		\hrule
		\vspace*{2mm}

		\subsection*{Exercise 6}
		Suppose $V$ is finite-dimensional, $T \in \Ell(V)$ has $\dim V$ distinct eigenvalues, and $S \in \Ell(V)$ has the same eigenvectors as $T$ (not necessarily with the same eigenvalues). 
		Prove that $ST = TS$.
		\vspace*{3mm}

		\begin{proof}
		Let $V,T,S$ be as above.
		Notice that $ST$ and $TS$ are linear operators, i.e., functions with some structure.
		Recall that to show equality of functions, we have to prove that they have the same domain and codomain, and that for a given input, the two functions produce the same output.
		Since $\Ell(V)$ is closed under the multiplication of linear operators, we know by definition that $ST$ and $TS$ have the same domain and codomain, namely $V$. Therefore, to show that they are equal, it remains to show that for a given input, they have the same output.

		Let $\dim V = n$.
		Further, let $\lambda_{1}, \dots, \lambda_{n}$ be the $n$ distinct eigenvalues of $T$, with $v_{1}, \dots, v_{n}$ their corresponding eigenvectors, i.e., for $j = 1,\dots,n$ we have $$Tv_{j} = \lambda_{j}v_{j}.$$
		Recall that these eigenvectors $v_{1}, \dots, v_{n}$ are also the eigenvectors of $S$, let their corresponding eigenvalues for $S$ be $\mu_{j}$ for $j=1,\dots,n$.
		Theorem 5.44 tells us that since $T$ has $n = \dim V$ distinct eigenvalues, we know $T$ is diagonalizable.
		Then we know $v_{1}, \dots, v_{n}$ form a basis of $V$ by Theorem 5.41. 
		So, let $v \in V$ and write $v$ in terms of this basis of eigenvectors, i.e., $v = a_{1}v_{1} + \cdots + a_{n}v_{n}$.

		\noindent{Next, we'll compute $STv$ and $TSv$; they should be equal.}
			\begin{align*}
				STv &= ST(a_{1}v_{1} + \cdots + a_{n}v_{n}), \\
				&= S ( a_{1}Tv_{1} + \cdots + a_{n}Tv_{n} ), \\
				&= S ( a_{1}\lambda_{1}v_{1} + \cdots + a_{n}\lambda_{n}v_{n} ), \\
				&= a_{1}\lambda_{1}Sv_{1} + \cdots + a_{n}\lambda_{n}Sv_{n}, \\
				&= a_{1}\lambda_{1}\mu_{1}v_{1} + \cdots + a_{n}\lambda_{n}\mu_{n}v_{n}.
			\end{align*}
		To clean up our scalars, let $\omega_{j} = a_{j}\lambda_{j}\mu_{j}$ for $j = 1,\dots,n$.
		Then we have $$STv = \omega_{1}v_{1} + \cdots + \omega_{n}v_{n}.$$
		Now we'll compute $TSv$, and hope it is equal to what we found above.
			\begin{align*}
				TSv &= TS(a_{1}v_{1} + \cdots + a_{n}v_{n}), \\
				&= T ( a_{1}Sv_{1} + \cdots + a_{n}Sv_{n} ), \\
				&= T ( a_{1}\mu_{1}v_{1} + \cdots + a_{n}\mu_{n}v_{n} ), \\
				&= a_{1}\mu_{1}Sv_{1} + \cdots + a_{n}\mu_{n}Sv_{n}, \\
				&= a_{1}\mu_{1}\lambda_{1}v_{1} + \cdots + a_{n}\mu_{n}\lambda_{n}v_{n}, \\
				&= a_{1}\lambda_{1}\mu_{1}v_{1} + \cdots + a_{n}\lambda_{n}\mu_{n}v_{n}, \\
				&= \omega_{1}v_{1} + \cdots + \omega_{n}v_{n}
			\end{align*}
		Hence, by the commutivity of scalar multiplication, we have shown $STv = TSv$, which implies $ST = TS$, as we aimed to do.
		\end{proof}
		% subsection exercise_6 (end)
		\vspace*{2mm}
		\hrule
		\vspace*{2mm}

		\subsection*{Exercise 8}
		Suppose $T \in \Ell(\F^{5})$ and $\dim{E(8, T)} = 4$.
		Prove that $T-􏰋2I$ or $T-􏰋6I$ is invertible.
		\vspace*{3mm}

		\begin{proof}
		Let $T$ be as above.
		We are given that $\dim{E(8, T)} = 4$, hence we know that $\lambda_{1} = 8$ is an eigenvalue of $T$ with four corresponding nonzero eigenvectors $v_{1}, v_{2}, v_{3}, v_{4}$.
		Moreover, since $\dim \F^{5} = 5$, Theorem 5.13 tells us that there are at most four more possible eigenvalues of $T$.
		Notice that by Theorem 5.6, to prove that $T-􏰋2I$ or $T-􏰋6I$ is invertible, it will be sufficient to show that none of these four possible eigenvalues are equal to 2 or 6.

		Suppose by way of contradiction that $\lambda_{2} = 2$ and $\lambda_{3} = 6$ are eigenvalues of $T$ with correspoding nonzero eigenvectors $x_{2}, x_{3}$.
		Then, by definition we know $E(2,T) \neq \{ 0 \}$ and $E(6,T) \neq \{ 0 \}$; it follows that each of their dimensions is at least 1.
		By Theorem 5.38, and since $\lambda_{1}, \lambda_{2}, \lambda_{3}$ are distinct eigenvalues of $T$, it follows that the sum of the dimensions of their correspoding eigenspaces is less than the dimension of $V$, i.e., $$\dim{E(2,T)} + \dim{E(6,T)} + \dim{E(8,T)} \leq \dim{V}.$$
		But this would imply that $1 + 1 + 4 \leq 5 \, \lightning$
		Hence, $2$ and $6$ are not eigenvalues of $T$, which implies that $T-􏰋2I$ or $T-􏰋6I$ is invertible, as we aimed to prove. 
		\end{proof}
		% subsection exercise_8 (end)
		\vspace*{2mm}
		\hrule
		\vspace*{2mm}

		\subsection*{Exercise 12}
		Suppose $R,T \in \Ell(\F^{3})$ each have 2, 6, 7 as eigenvalues. 
		Prove that there exists an invertible operator $S \in \Ell(\F^{3})$ such that $R = S^{-1}TS$.
		\vspace*{3mm}

		\begin{proof}
		Let $R,T$ be as above.
		Then $\lambda_{1} = 2$, $\lambda_{1} = 2$, and $\lambda_{1} = 2$ are the eigenvalues of $R$ and $T$, with corresponding eigenvectors $v_{1}, v_{2}, v_{3}$ for $T$, and $w_{1}, w_{2}, w_{3}$ for $R$, i.e.,
			\begin{align*}
				&Tv_{1} = 2v_{1}, \hspace{5mm} Tv_{2} = 6v_{2}, \hspace{5mm} Tv_{3} = 7v_{3}; \\
				&Rw_{1} = 2v_{1}, \hspace{5mm} Rw_{2} = 6v_{2}, \hspace{5mm} Rw_{3} = 7v_{3}.
			\end{align*}
		Define $S \in \Ell(V)$ by $w_{j} \mapsto v_{j}$ for $j = 1,2,3$.
		Notice that the definition of $S$ establishes a one-to-one correspondence between different basis elements of $V$, therefore $S$ is bijective and invertible.
		It follows that we have the inverse operator $S^{-1} \in \Ell(V)$ defined by $v_{j} \mapsto w_{j}$ for $j = 1,2,3$.
		As in Exercise 5.C.6, $R$ and $S^{-1}TS$ have the same domain and codomain, so to show $R = S^{-1}TS$, it will suffice to show that for the same input, the two operators produce the same output.
		Recall, since $R$ and $T$ each have $\dim V$ distinct eigenvalues, by Theorems 5.44 and 5.41, we know that they are diagonalizable, and that their eigenvectors form bases of $V$.
		Let $w \in V$ such that $w = b_{1}w_{1} + b_{2}w_{2} + b_{3}w_{3}$.
		We will show that $S^{-1}TSw = Rw$ for our arbitrarily chosen $w$.
			\begin{align*}
				S^{-1}TSw &= S^{-1}TS(b_{1}w_{1} + b_{2}w_{2} + b_{3}w_{3}), \\
				&= S^{-1} T(b_{1}Sw_{1} + b_{2}Sw_{2} + b_{3}Sw_{3}), \\
				&= S^{-1} T(b_{1}v_{1} + b_{2}v_{2} + b_{3}v_{3}), \\
				&= S^{-1}(b_{1}Tv_{1} + b_{2}Tv_{2} + b_{3}Tv_{3}),
			\end{align*}
			\begin{align*}
				&= S^{-1}(2b_{1}v_{1} + 6b_{2}v_{2} + 6b_{3}v_{3}), \\
				&= 2b_{1}S^{-1}v_{1} + 6b_{2}S^{-1}v_{2} + 7b_{3}S^{-1}v_{3}, \\
				&= 2b_{1}w_{1} + 6b_{2}w_{2} + 7b_{3}w_{3}.
			\end{align*}
		Now we'll show that this equals $Rw$.
			\begin{align*}
				Rw &= R(b_{1}w_{1} + b_{2}w_{2} + b_{3}w_{3}), \\
				&= b_{1}Rw_{1} + b_{2}Rw_{2} + b_{3}Rw_{3}, \\
				&= b_{1}2w_{1} + b_{2}6w_{2} + b_{3}7w_{3}, \\
				&= 2b_{1}w_{1} + 6b_{2}w_{2} + 7b_{3}w_{3}.
			\end{align*}
		Notice that we could just flip some of this stuff around and show our desired result this way,
			\begin{align*}
				S^{-1}TSw &= S^{-1}TS(b_{1}w_{1} + b_{2}w_{2} + b_{3}w_{3}), \\
				&= S^{-1} T(b_{1}Sw_{1} + b_{2}Sw_{2} + b_{3}Sw_{3}), \\
				&= S^{-1} T(b_{1}v_{1} + b_{2}v_{2} + b_{3}v_{3}), \\
				&= S^{-1}(b_{1}Tv_{1} + b_{2}Tv_{2} + b_{3}Tv_{3}), \\
				&= S^{-1}(2b_{1}v_{1} + 6b_{2}v_{2} + 6b_{3}v_{3}), \\
				&= 2b_{1}S^{-1}v_{1} + 6b_{2}S^{-1}v_{2} + 7b_{3}S^{-1}v_{3}, \\
				&= 2b_{1}w_{1} + 6b_{2}w_{2} + 7b_{3}w_{3}, \\
				&= b_{1}2w_{1} + b_{2}6w_{2} + b_{3}7w_{3}, \\
				&= b_{1}Rw_{1} + b_{2}Rw_{2} + b_{3}Rw_{3}, \\
				&= R(b_{1}w_{1} + b_{2}w_{2} + b_{3}w_{3}), \\
				&= Rw.
			\end{align*}
		This is probably `more elegant,' but who's to say.
		In either case, we have shown $S^{-1}TSw = Rw$, as we aimed to do.
		\end{proof}

		% subsection exercise_12 (end)
		\vspace*{2mm}
		\hrule
		\vspace*{2mm}

		\subsection*{Exercise 14}
		Find $T \in \Ell(\C^{3})$ such that 6 and 7 are eigenvalues of $T$ and such that $T$ does not have a diagonal matrix with respect to any basis of $\C^{3}$.
		\vspace*{3mm}

		\begin{proof}
		Let $T$ be as above, notice $\dim \C^{3} = 3$.
		First we should understand how we're going to handle the statement ``$T$ does not have a diagonal matrix with respect to any basis of $\C^{3}$.''
		Theorem 5.41 states that this is equivalent to $\dim \C^{3} = \dim E(\lambda_{1},T) + \dim E(\lambda_{2},T) + \dim E(\lambda_{3},T)$ being false.
		Therefore, if we can cook up an operator $T$ that breaks the dimension part of Theorem 5.41, then we're done.
		Recall that we are also told to assume that $\lambda_{1} = 6$, and $\lambda_{2} = 7$.
		Since these are nonzero eigenvalues, they have corresponding eigenspaces with dimension at least 1.
		This means that however we make $T$, it has to be such that $\lambda_{1}$ and $\lambda_{2}$ are its only eigenvalues.
		Moreover, we have to make sure that they each have exactly one corresponding eigenvector, otherwise the dimensions of their eigenspaces could sum to 3, which we need to avoid.
		The best way to do this will be to create a map that corresponds to an upper-triangular matrix with only 6's and 7's along the diagonal.
		Let $\zeta_{1},\zeta_{2},\zeta_{3}$ be a basis of $\C^{3}$, then define $T \in \Ell(\C^{3})$ such that $(\zeta_{1},\zeta_{2},\zeta_{3}) \mapsto (6\zeta_{1} + \zeta_{2}, 6\zeta_{2}, 7\zeta_{3})$.
		Clearly, the matrix of $T$ is $$\mathcal{M}(T,(\zeta_{1}, \dots, \zeta_{n})) = \begin{pmatrix} 6 & 1 & 0 \\ 0 & 6 & 0 \\ 0 & 0 & 7 \end{pmatrix}.$$
		Notice that $\mathcal{M}(T,(\zeta_{1}, \dots, \zeta_{n}))$ is upper-triangular, so it follows by Theorem 5.32 that the eigenvalues of $T$ are precisely $\lambda_{1} = 6$, and $\lambda_{2} = 7$; notice that $\lambda_{1}$ has multiplicity two.
		To finish the proof, we just have to check that the corresponding eigenspaces for $\lambda_{1}$ and $\lambda_{2}$ are each dimension one.
		It is clear by definitions that $\dim E(7,T) = 1$, so it remains to check that $\dim E(6,T) < 2$.
		Consider the linear map $T - 6I$ and its isomorphic matrix $$\mathcal{M}(T - 6I, (\zeta_{1},\zeta_{2},\zeta_{3})) = \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}.$$
		Jumping back from $\C^{3,3}$ to $\Ell(\C^3)$ and we see that $T - 6I$ is defined by the mapping $(\zeta_{1},\zeta_{2},\zeta_{3}) \mapsto (\zeta_{2},0,\zeta_{3})$.
		Since $(T - 6I)\zeta \in \range T - 6I$ it follows that $\dim \range T - 6I = 2$.
		Since the dimensions of the null space and range of $T$ must sum to 3 in this case, we know $\dim \null T - 6I = \dim E(6,T) = 1$.			
		It follows that we have $\dim E(6,T) = 1$ and $\dim E(7,T) = 1$, clearly these do not sum to 3.
		Thus, by Theorem 5.41, since $\dim \C^3 \neq \dim E(6,T) + \dim E(7,T)$, it is equivalent to state that $T$ does not have a diagonal matrix with respect to any basis of $\C^{3}$.
		\end{proof}
		% subsection exercise_14 (end)
		\vspace*{2mm}
		\hrule
		\vspace*{2mm}
	% section section_5_c (end)

	\section*{Section 8.A}
		\subsection*{Exercise 2}
		Define $T \in \Ell(\C^2)$ by $$T(w,z) = (-z,w).$$
		Find the generalized eigenspaces corresponding to the distinct eigenvalues of $T$.

		\begin{proof}
		Let $T$ be as above.
		Notice $\dim \C^2 = 2$, so we are looking for exactly two eigenvalues, with corresponding one-dimensional generalized eigenspaces.
		Recall that we've seen this operator before, it rotates things in the complex plane by $\pm\pi/2$.
		Leaning on some knowledge from geometry, we know that rotation by $\pm\pi/2$ in the complex plane is achieved via multiplication by $\pm i$.
		It follows that these are the two eigenvalues of $T$, so by Theorem 8.11 we can define the corresponding generalized eigenspaces thus, $$G(\pm i,T) = \null(T \pm iI)^2.$$			
		\end{proof}
		% subsection exercise_2 (end)
		\vspace*{2mm}
		\hrule
		\vspace*{2mm}

		\subsection*{Exercise 6}
		Suppose $T \in \Ell(\C^3)$ is defined by $T(z_{1}, z_{2}, z_{3}) = (z_{2},z_{3},0)$. 
		Prove that $T$ has no square root. 
		More precisely, prove that there does not exist $S \in \Ell(\C^3)$ such that $S^2=T$.

		\begin{proof}
		Let $T$ be as above, and suppose by way of contradiction that there exists $S \in \Ell(\C^3)$ such that $S^2=T$.
		Notice that $T^{3}(z_{1}, z_{2}, z_{3}) = (0,0,0)$, so it follows that $V = \null T^3$.

		\noindent{Moreover, by Theorem 8.4 we have $\null T^3 = \null T^{3+k}$ for $k \in \Z^{+}$.
		Now let's go contradiction hunting.}
			\begin{align*}
				V &= \null T^3, \\
				&= \null S^6, \\
				&= \null S^3, \\
				&= \null S^2S, \\
				&= \null TS.
			\end{align*}
		Now, we know $\null TS \subset \null TS^2$, which is equivalent to $V \subset \null T^2$.
		Notice that $T^2(z_{1}, z_{2}, z_{3}) = (z_{3},0,0)$, so $\null T^2 = \{ (0,0,z) \}$.
		Thus, we're asserting $\C^3 \subset \{ (0,0,z) \} \, \lightning$
		Thus, it follows that $T$ has no square root.
		\end{proof}
		% subsection exercise_6 (end)
		\vspace*{2mm}
		\hrule
		\vspace*{2mm}

		\subsection*{Exercise 12}
		Suppose $N \in \Ell(V)$ and there exists a basis of $V$ with respect to which $N$ has an upper-triangular matrix with only $0$'s on the diagonal. 
		Prove that $N$ is nilpotent.

		\begin{proof}
		Let $N \in \Ell(V)$ so that $\mathcal{M}(N, (v_{1},\dots,v_{n}))$ is the upper-triangular matrix described above.
		It follows from definitions that $v_{1}$ is a vector of 0's, and that $\dim V = n$.
		Then $Nv_{1} = 0$.
		From the definitions of matrices of linear maps, we know that $N^2v_{2} \in \Span{(v_{1})}$.
		Since $\dim(\Span(v_{1})) = 1$, and $\Span(v_{1}) \in \Ell(V)$, it follows by Theorem 8.4 that $N^2v_2 = 0$.
		Similarly, we have $Nv_{3} \in \Span{(v_{1},v_{2})}$ and $N^{3}v_{3} = 0$.
		Continuing down the integers we see that $N^jv_j = 0$ until we arrive at $N^n=0$.
		This satisfies the definition of a nilpotent operator on an $n$-dimensional vector space, hence $N$ is nilpotent as we aimed to prove.
		\end{proof}
		% subsection exercise_12 (end)
		\vspace*{2mm}
		\hrule
		\vspace*{2mm}

		\subsection*{Exercise 17}
		Suppose $T \in \Ell(V)$ and $m$ is a nonnegative integer such that $$\range T^{m} = \range T^{m+1}.$$
		Prove that $\range T^k = \range T^m$ for all $k > m$.

		\begin{proof}
		Let $T \in \Ell(V)$ and $m \in \Z^{+}$ such that $\range T^m = \range T^{m+1}$.
		Recall that we assume in Chapter 8 that $V$ is finite dimensional, so, let $n = \dim V$.
		By Theorem 8.5 we know that $V = \null T^n  \, \oplus \, \range T^n$.
		Further, in Exercise 5.C.3., we proved that this is equivalent to $V = \null T^n + \range T^n$.
		Applying the Fundamental Theorem of Linear Maps yields that $$n = \dim \null T^n + \dim \range T^n.$$
		Moreover, from Theorem 8.4, we know that this is a point where the null space stops growing, i.e., $\null T^{m} = \null T^{k}$ for all $k > m$ when $m = n$.
		Since the dimensions of $\null T^{m}$ and $\range T^{m}$ must sum to $n$, it follows that when $m = n$ we also have $\range T^{m} = \range T^{k}$ when $m = n$.

		By this argument, it follows that whenever the range or null space of $T$ ceases to grow, the FTLM requires that the other ceases to grow as well.
		We assume that $\range T^{m} = \range T^{m+1}$.
		The FTLM requires that $\null T^{m} = \null T^{m+1}$, this triggers Theorem 8.3, which tells us that $\null T^m = \null T^{m+1}$ implies $\null T^m = \null T^k$ for each $k \geq m$.
		Theorem 8.4 tells us that this definitely happens when $m = n$, but this could happen for $m \leq n$, so long as $m$ is such that $\range T^{m} = \range T^{m+1}$.
		\end{proof}
		% subsection exercise_17 (end)
	% section section_8_a (end)
\end{document}